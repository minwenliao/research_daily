# 具身智能

## 从Diffusion Policy: Visuomotor Policy Learning via Action Diffusion开始


1. Dp为action， 为了保证动作的时序一致性和平滑性，这里让dp预测了一段固定间隔的动作序列。

2. 视觉观测作为condition，条件分布可以表达为  ，相比于联合分布  ，观测作为条件能够加速去燥过程，并且提高生成动作的准确性。（  是t时刻输出的动作序列）
## DIT

## ACT
整体架构：
CVAE + Transformer，带 KL 正则与风格变量 z

 Temporal Ensemble 的核心：
```python
exp_weights = np.exp(-k * np.arange(len(actions_for_curr_step)))
```
```python
raw_action = (actions_for_curr_step * exp_weights).sum(axis=0)
```
✅ 解决 chunk 切换时的不平滑（避免动作突跳）
✅ 保留多次预测的冗余性（某次预测抖动不至于完全决定输出）
✅ 越靠近当前观测的预测越可信 → 加权平均是有意义的

感知输入：

```python
obs['images'] = {
    'cam_high': img_front, 'cam_left_wrist': img_left, 'cam_right_wrist': img_right
}
obs['qpos'] = concat(puppet_arm_left.position + right.position + base_vel)
```

```python
ros_operator.puppet_arm_publish_continuous_thread(left_action, right_action)
```

$$
\log p(x) = \log \int p(x|z)p(z) \, dz
$$


## RDT

## Pi0

##
