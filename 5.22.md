# 具身智能
###  action + observatiobn


## 从Diffusion Policy: Visuomotor Policy Learning via Action Diffusion开始


1. Dp为action， 为了保证动作的时序一致性和平滑性，这里让dp预测了一段固定间隔的动作序列。

2. 视觉观测作为condition，条件分布可以表达为  ，相比于联合分布  ，观测作为条件能够加速去燥过程，并且提高生成动作的准确性。（  是t时刻输出的动作序列）
## DIT

## ACT
整体架构：
CVAE + Transformer，带 KL 正则与风格变量 z

 

感知输入：

```python
obs['images'] = {
    'cam_high': img_front, 'cam_left_wrist': img_left, 'cam_right_wrist': img_right
}
obs['qpos'] = concat(puppet_arm_left.position + right.position + base_vel)
```

```python
ros_operator.puppet_arm_publish_continuous_thread(left_action, right_action)
```

### 长 chunk 会导致：
更大输出张量（比如输出 k×14）
更难训练和收敛，尤其在多模态动作场景中（容易学不到细节）
模型误差会累积在 chunk 内

虽然 ACT 是为了解决 compounding error（逐帧预测时的误差积累）
但 chunk 越长，一次性预测的内容越多，内部误差更难被修正
而且中间每一步都没有新的环境感知（obs），预测会逐渐脱离实际状态

为了解决chunk带来的误差，act中引入了Temporal Ensemble，它的核心：
```python
exp_weights = np.exp(-k * np.arange(len(actions_for_curr_step)))
```
```python
raw_action = (actions_for_curr_step * exp_weights).sum(axis=0)
```
✅ 解决 chunk 切换时的不平滑（避免动作突跳）
✅ 越靠近当前观测的预测越可信 → 加权平均是有意义的
### CVAE解析：

$$
\log p(x) = \log \int p(x|z)p(z) \, dz
$$


## RDT

## Pi0

##
